{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9491a02-03dc-479e-9898-ccbd40377617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "\n",
    "MAX_VOCAB_SIZE = 2000  \n",
    "MAX_LEN = 12           \n",
    "BATCH_SIZE = 8         \n",
    "EPOCHS = 10         \n",
    "LATENT_DIM = 128        \n",
    "\n",
    "sentences_df = pd.read_csv('sentences.csv', sep='\\t', names=['id', 'lang', 'text'])\n",
    "eng_df = sentences_df[sentences_df['lang'] == 'eng']\n",
    "spa_df = sentences_df[sentences_df['lang'] == 'spa']\n",
    "links_df = pd.read_csv('links.csv', sep='\\t', names=['eng_id', 'spa_id'])\n",
    "\n",
    "eng_spa = links_df.merge(eng_df, left_on='eng_id', right_on='id').merge(\n",
    "    spa_df, left_on='spa_id', right_on='id', suffixes=('_eng', '_spa'))\n",
    "\n",
    "english_texts = eng_spa['text_eng'].astype(str).tolist()\n",
    "spanish_texts = eng_spa['text_spa'].astype(str).tolist()\n",
    "\n",
    "max_samples = 5000\n",
    "english_texts = english_texts[:max_samples]\n",
    "spanish_texts = spanish_texts[:max_samples]\n",
    "\n",
    "filtered_pairs = [(e, s) for e, s in zip(english_texts, spanish_texts)\n",
    "                  if len(e.split()) <= MAX_LEN and len(s.split()) <= MAX_LEN]\n",
    "english_texts, spanish_texts = zip(*filtered_pairs)\n",
    "\n",
    "START_TOKEN = '<start>'\n",
    "END_TOKEN = '<end>'\n",
    "spanish_texts = [f\"{START_TOKEN} {txt} {END_TOKEN}\" for txt in spanish_texts]\n",
    "\n",
    "def build_tokenizer(texts, num_words=None):\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(num_words=num_words, filters='')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "eng_tokenizer = build_tokenizer(english_texts, num_words=MAX_VOCAB_SIZE)\n",
    "spa_tokenizer = build_tokenizer(spanish_texts, num_words=MAX_VOCAB_SIZE)\n",
    "\n",
    "num_encoder_tokens = min(len(eng_tokenizer.word_index) + 1, MAX_VOCAB_SIZE)\n",
    "num_decoder_tokens = min(len(spa_tokenizer.word_index) + 1, MAX_VOCAB_SIZE)\n",
    "max_encoder_seq_length = min(MAX_LEN, max(len(txt.split()) for txt in english_texts))\n",
    "max_decoder_seq_length = min(MAX_LEN + 2, max(len(txt.split()) for txt in spanish_texts))  # +2 for start/end\n",
    "\n",
    "encoder_input_data = eng_tokenizer.texts_to_sequences(english_texts)\n",
    "encoder_input_data = keras.preprocessing.sequence.pad_sequences(\n",
    "    encoder_input_data, maxlen=max_encoder_seq_length, padding='post')\n",
    "\n",
    "decoder_input_data = spa_tokenizer.texts_to_sequences(spanish_texts)\n",
    "decoder_input_data = keras.preprocessing.sequence.pad_sequences(\n",
    "    decoder_input_data, maxlen=max_decoder_seq_length, padding='post')\n",
    "\n",
    "decoder_target_data = np.zeros((len(english_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "for i, seq in enumerate(decoder_input_data):\n",
    "    for t in range(1, len(seq)):\n",
    "        if seq[t] < num_decoder_tokens:\n",
    "            decoder_target_data[i, t - 1, seq[t]] = 1.0\n",
    "\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(num_encoder_tokens, LATENT_DIM)(encoder_inputs)\n",
    "encoder_lstm = LSTM(LATENT_DIM, return_state=True)\n",
    "_, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, LATENT_DIM)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.2)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
    "decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + [state_h2, state_c2])\n",
    "\n",
    "reverse_spa_index = {idx: word for word, idx in spa_tokenizer.word_index.items()}\n",
    "reverse_spa_index[0] = '' \n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = spa_tokenizer.word_index.get(START_TOKEN, 1)\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_spa_index.get(sampled_token_index, '')\n",
    "        if (sampled_word == END_TOKEN or len(decoded_sentence.split()) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            decoded_sentence += ' ' + sampled_word\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence.strip()\n",
    "\n",
    "def translate(sentence):\n",
    "    seq = eng_tokenizer.texts_to_sequences([sentence])\n",
    "    seq = keras.preprocessing.sequence.pad_sequences(seq, maxlen=max_encoder_seq_length, padding='post')\n",
    "    return decode_sequence(seq)\n",
    "\n",
    "print(translate('Hello')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242097b9-1f0a-4494-9854-2100726bdef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
